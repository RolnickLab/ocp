# This is the exact same configs as in is2re_aux_cst_5i_1w_noearlystop
# but with updated code for 
# lmdbdataset.py, where we actually noise all graphs of the dataset
# whereas previously (noisy_nodes_noisy_lmdb branch) 
# we were just noising one graph of the dataset. Thus, we were essentially doing
# ISR2S because of the position prediction head and the auxiliary loss which is MSE
# -------------------------------
# Initially batch size wad 256 but caused only OOMs, so changed it to 128 in this same file
job:
  mem: 80GB
  cpus: 4
  gres: gpu:1
  partition: long
  # time: 15:00:00

default:
  mode: train
  optim:
    batch_size: 128
    eval_batch_size: 128
    optimizer: AdamW
    auxiliary_decay: False
    auxiliary_task_weight: 1
    energy_coefficient: 0.0
    max_epochs: 50
  cp_data_to_tmpdir: True
  normalizer: null
  model:
    otf_graph: False
    max_num_neighbors: 40
    noisy_nodes: True
    num_interactions: 12
  trainer: single
  logger: wandb
  early_stop: False

  frame_averaging: 2D
  fa_method: se3-random

  dataset:
    train:
      noisy_nodes:
        type: constant #rand or rand_deter or constant
        interpolate_threshold: 0.5
        min_interpolate_factor: 0.0
        gaussian_noise_std: 0.3

  task:
    dataset: lmdb_noisy #single_point_lmdb  
    description: "Relaxed state energy prediction from initial structure."
    type: regression
    metric: mae
    labels:
      - relaxed energy

# ----------------------all dataset-------------------
# CosineAnnealing
runs:
  - config: faenet-is2re_aux-all
    note: "Cosine-very-low-lr"
    optim:
      lr_initial: 0.0001
      scheduler: LinearWarmupCosineAnnealingLR

  - config: faenet-is2re_aux-all
    note: "Cosine-low-lr"
    optim:
      lr_initial: 0.0005
      scheduler: LinearWarmupCosineAnnealingLR
  
  - config: faenet-is2re_aux-all
    note: "Cosine-med-lr"
    optim:
      lr_initial: 0.001
      scheduler: LinearWarmupCosineAnnealingLR

  - config: faenet-is2re_aux-all
    note: "Cosine-usual-lr"
    optim:
      lr_initial: 0.002
      scheduler: LinearWarmupCosineAnnealingLR

# Null LR
  - config: faenet-is2re_aux-all
    note: "null-very-low-lr"
    optim:
      lr_initial: 0.0001
      scheduler: Null

  - config: faenet-is2re_aux-all
    note: "null-low-lr"
    optim:
      lr_initial: 0.0005
      scheduler: Null
  
  - config: faenet-is2re_aux-all
    note: "null-med-lr"
    optim:
      lr_initial: 0.001
      scheduler: Null

  - config: faenet-is2re_aux-all
    note: "null-usual-lr"
    optim:
      lr_initial: 0.002
      scheduler: Null
# ----------------------------10k-----------------------
# CosineAnnealing
  - config: faenet-is2re_aux-10k
    note: "Cosine-very-low-lr"
    optim:
      lr_initial: 0.000002
      scheduler: LinearWarmupCosineAnnealingLR
      max_epochs: 500

  - config: faenet-is2re_aux-10k
    note: "Cosine-low-lr"
    optim:
      lr_initial: 0.00001
      scheduler: LinearWarmupCosineAnnealingLR
      max_epochs: 500
  
  - config: faenet-is2re_aux-10k
    note: "Cosine-med-lr"
    optim:
      lr_initial: 0.00002
      scheduler: LinearWarmupCosineAnnealingLR
      max_epochs: 500

  - config: faenet-is2re_aux-10k
    note: "Cosine-usual-lr"
    optim:
      lr_initial: 0.00004
      scheduler: LinearWarmupCosineAnnealingLR
      max_epochs: 500

# Null LR
  - config: faenet-is2re_aux-10k
    note: "null-very-low-lr"
    optim:
      lr_initial: 0.000002
      scheduler: Null
      max_epochs: 500

  - config: faenet-is2re_aux-10k
    note: "null-low-lr"
    optim:
      lr_initial: 0.00001
      scheduler: Null
      max_epochs: 500
  
  - config: faenet-is2re_aux-10k
    note: "null-med-lr"
    optim:
      lr_initial: 0.00002
      scheduler: Null
      max_epochs: 500

  - config: faenet-is2re_aux-10k
    note: "null-usual-lr"
    optim:
      lr_initial: 0.00004
      scheduler: Null
      max_epochs: 500
